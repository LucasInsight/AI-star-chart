"""
ç‰ˆæƒæ‰€æœ‰ 2024 AIæ˜Ÿå›¾

æ ¹æ® Apache License, Version 2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›
é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™ä½ ä¸èƒ½ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯å‰¯æœ¬ï¼š

    http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æŒ‰â€œåŸæ ·â€æä¾›ï¼Œ
ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–ä¿è¯ã€‚
è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£ç®¡ç†æƒé™å’Œé™åˆ¶çš„ç‰¹å®šè¯­è¨€ã€‚

é™„åŠ æ¡æ¬¾ï¼š
- æœ¬ä½œå“çš„å‰¯æœ¬å¯ä»¥è‡ªç”±å¤åˆ¶å’Œåˆ†å‘ï¼Œä½†ä¸å¾—è¿›è¡Œä¿®æ”¹ã€è¡ç”Ÿæˆ–æ¼”ç»ã€‚
- å¦‚éœ€è½¬è½½æˆ–å…¶ä»–ç”¨é€”ï¼Œå¿…é¡»äº‹å…ˆè·å¾— AIæ˜Ÿå›¾æƒåˆ©äººçš„ä¹¦é¢è®¸å¯ã€‚

---

Copyright 2024 AIæ˜Ÿå›¾

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

Additional Terms:
- Copies of this work may be freely copied and distributed, but modification, derivation, or adaptation is not allowed.
- For any form of redistribution or other use, prior written permission from the copyright holder, AI

"""

# -*- coding: utf-8 -*-
"""AIæ˜Ÿå›¾-Llama-3.1-8B-Instruct-finetune-release

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XApK1lhFUPNg1vAkjEjPgg2Ba5yiChnq

**æ¬¢è¿æ¥åˆ°AIæ˜Ÿå›¾**

æœ¬æœŸè¯¾ç¨‹ï¼Œæˆ‘ä»¬å¯¹Llama3.1è¿›è¡Œä¸­æ–‡æ•°æ®é›†å¾®è°ƒï¼Œå¹¶è§£å†³å®é™…å¾®è°ƒä¸­é‡åˆ°çš„é—®é¢˜ï¼Œç¡®ä¿å¾®è°ƒè¾¾åˆ°é¢„æœŸçš„æ•ˆæœã€‚

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
  <a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://ollama.com/"><img src="https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/ollama.png" height="44"></a>
  <a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
  <a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Join Discord if you need help + â­ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> â­
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).

You will learn how to do [data prep](#Data) and import a CSV, how to [train](#Train), how to [run the model](#Inference), & [how to export to Ollama!](#Ollama)

[Unsloth](https://github.com/unslothai/unsloth) now allows you to automatically finetune and create a [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md), and export to [Ollama](https://ollama.com/)! This makes finetuning much easier and provides a seamless workflow from `Unsloth` to `Ollama`!

**[NEW]** We now allow uploading CSVs, Excel files - try it [here](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) by using the Titanic dataset.

**è§†é¢‘è¯¾ç¨‹æ¦‚è¦ï¼šå¯¹Llama3.1è¿›è¡Œä¸­æ–‡æ•°æ®é›†å¾®è°ƒï¼Œæ”¯æŒOllamaè¿è¡Œ**  
**Video Course Outline: Fine-Tuning Llama3.1 with Chinese Dataset for Ollama Deployment**

**è¯¾ç¨‹ç®€ä»‹ï¼š**  
**Course Introduction:**  
æœ¬è¯¾ç¨‹åŸºäºunslothå°†å¸¦é¢†è§‚ä¼—æ·±å…¥äº†è§£å¦‚ä½•åœ¨ä½é…CPUç¯å¢ƒä¸‹ï¼Œå¯¹Llama3.1-8B-Instructå¤§æ¨¡å‹è¿›è¡Œä¸­æ–‡æ•°æ®é›†çš„æŒ‡ä»¤å¾®è°ƒï¼Œå¹¶æˆåŠŸè¿è¡ŒäºOllamaå¼•æ“ä¸­ã€‚é€šè¿‡æœ¬è¯¾ç¨‹ï¼Œè§‚ä¼—å°†æŒæ¡æ¨¡å‹å¾®è°ƒçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œè§£å†³å¸¸è§çš„å¾®è°ƒè¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œå¹¶æœ€ç»ˆå®ç°æ¨¡å‹åœ¨Ollamaç¯å¢ƒä¸‹çš„é«˜æ•ˆè¿è¡Œã€‚  
This course will guide viewers through the process of fine-tuning the Llama3.1-8B-Instruct model with a Chinese dataset using unsloth, even in low-spec CPU environments, and successfully running it on the Ollama engine. Viewers will learn core techniques for model fine-tuning, troubleshoot common issues, and achieve efficient model deployment on the Ollama platform.

**è¯¾ç¨‹ç›®æ ‡ï¼š**  
**Course Objectives:**  
1. **ä½é…GPUç¯å¢ƒä¸‹çš„æ¨¡å‹å¾®è°ƒï¼š** å­¦ä¹ å¦‚ä½•åœ¨èµ„æºæœ‰é™çš„CPUç¯å¢ƒä¸‹ï¼Œå¯¹Llama3.1-8B-Instructå¤§æ¨¡å‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œé€‚åº”æœ¬åœ°åŒ–çš„éœ€æ±‚ã€‚  
   **Model Fine-Tuning in Low-Spec GPU Environments:** Learn how to efficiently fine-tune the Llama3.1-8B-Instruct model in resource-constrained CPU environments, particularly with a Chinese dataset for instruction-based fine-tuning to meet localization needs.

2. **æ”¯æŒOllamaå¼•æ“çš„å¾®è°ƒæ¨¡å‹è¿è¡Œï¼š** æŒæ¡å¦‚ä½•åœ¨å¾®è°ƒå®Œæˆåï¼Œå°†æ¨¡å‹åŠ è½½å¹¶è¿è¡ŒäºOllamaå¼•æ“ä¸­ï¼Œç¡®ä¿æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å…¼å®¹æ€§å’Œæ€§èƒ½ã€‚  
   **Running Fine-Tuned Models on Ollama Engine:** Master how to load and run the fine-tuned model on the Ollama engine, ensuring compatibility and performance in practical applications.

**è¯¾ç¨‹å†…å®¹ï¼š**  
**Course Content:**  
1. **Llama3.1-8B-Instructç‰ˆæœ¬ç®€ä»‹ä¸å¾®è°ƒåŸºç¡€ï¼š**  
   - ä»‹ç»Llama3.1-8B-Instructæ¨¡å‹çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚  
   - å¾®è°ƒçš„åŸºæœ¬åŸç†ä¸æ­¥éª¤æ¦‚è¿°ï¼Œç‰¹åˆ«æ˜¯æŒ‡ä»¤å¾®è°ƒçš„ç»†èŠ‚è®²è§£ã€‚  
   **Introduction to Llama3.1-8B-Instruct and Fine-Tuning Basics:**  
   - Overview of the Llama3.1-8B-Instruct modelâ€™s features and use cases.  
   - Basic principles and steps of fine-tuning, with detailed explanations on instruction-based fine-tuning.

2. **ä½¿ç”¨unslothè¿›è¡Œå¾®è°ƒçš„å®è·µï¼š**  
   - å¦‚ä½•é…ç½®ä½é…GPUç¯å¢ƒè¿›è¡Œå¾®è°ƒï¼Œå……åˆ†åˆ©ç”¨èµ„æºè¿›è¡Œä¼˜åŒ–ã€‚  
   - ä¸­æ–‡æ•°æ®é›†çš„å‡†å¤‡ä¸åŠ è½½ï¼Œç¡®ä¿æ•°æ®çš„æœ‰æ•ˆæ€§å’Œè¦†ç›–é¢ã€‚  
   **Practical Fine-Tuning with Unsloth:**  
   - How to configure a low-spec GPU environment for fine-tuning, optimizing resource use.  
   - Preparation and loading of the Chinese dataset to ensure data validity and coverage.

3. **é—®é¢˜æ’æŸ¥ä¸ä¿®å¤ï¼š**  
   - è§£å†³unslothå¾®è°ƒè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ç¨‹åºå´©æºƒé—®é¢˜ï¼Œæä¾›è¯¦ç»†çš„è°ƒè¯•æ–¹æ³•ã€‚  
   - å¤„ç†å¾®è°ƒåæ¨¡å‹å¤è¯»ã€ä¹±ç ã€interfaceä¸å…¼å®¹ã€æŒ‡ä»¤æœªæ¥æ”¶ç­‰å¸¸è§é—®é¢˜ï¼Œç¡®ä¿å¾®è°ƒæ•ˆæœè¾¾åˆ°é¢„æœŸã€‚  
   **Troubleshooting and Fixing Issues:**  
   - Resolving potential crashes during unsloth fine-tuning, with detailed debugging methods.  
   - Handling common issues post-fine-tuning such as model repetition, garbled text, interface incompatibility, and unprocessed instructions to ensure expected fine-tuning results.

4. **æ¨¡å‹åœ¨Ollamaå¼•æ“ä¸­çš„éƒ¨ç½²ä¸è¿è¡Œï¼š**  
   - ä»‹ç»Ollamaå¼•æ“çš„ç‰¹ç‚¹ä¸ä¼˜åŠ¿ã€‚  
   - å°†å¾®è°ƒåçš„æ¨¡å‹å¯¼å…¥Ollamaå¼•æ“ä¸­ï¼Œå¹¶è¿›è¡Œæµ‹è¯•å’Œä¼˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹çš„å…¼å®¹æ€§å’Œæ€§èƒ½ã€‚  
   **Deploying and Running Models on Ollama Engine:**  
   - Introduction to the features and advantages of the Ollama engine.  
   - Importing the fine-tuned model into the Ollama engine, followed by testing and optimization to ensure compatibility and performance.

5. **æ€»ç»“ä¸å±•æœ›ï¼š**  
   - å›é¡¾è¯¾ç¨‹çš„å…³é”®æ­¥éª¤ä¸çŸ¥è¯†ç‚¹ã€‚  
   - å±•æœ›Llama3.1å¤§æ¨¡å‹åœ¨ä¸šåŠ¡åœºæ™¯ä¸‹çš„åº”ç”¨å‰æ™¯å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚  
   **Summary and Outlook:**  
   - Recap of key steps and knowledge points from the course.  
   - Future prospects and developments of the Llama3.1 model in business scenarios.

é€šè¿‡æœ¬è¯¾ç¨‹çš„å­¦ä¹ ï¼Œè§‚ä¼—å°†èƒ½å¤Ÿåœ¨æœ‰é™çš„ç¡¬ä»¶æ¡ä»¶ä¸‹ï¼Œå¯¹Llama3.1è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œå¹¶æˆåŠŸéƒ¨ç½²åœ¨Ollamaå¼•æ“ä¸­ï¼Œè§£å†³æ¨¡å‹åº”ç”¨ä¸­çš„å®é™…é—®é¢˜ã€‚  
By the end of this course, viewers will be able to efficiently fine-tune Llama3.1 under limited hardware conditions and successfully deploy it on the Ollama engine, addressing practical issues in model applications.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Unsloth å®‰è£…
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes

"""* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc
* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.
* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.
* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.
* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)
"""

# æ¨¡å‹è£…è½½ å‚æ•°åˆå§‹åŒ–
from unsloth import FastLanguageModel
import torch
max_seq_length = 131072 # Llama3.1æ”¯æŒ128K # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 15 trillion tokens model 2x faster!
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # We also uploaded 4bit for 405b!
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # New Mistral 12b 2x faster!
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",          # Phi-3 2x faster!d
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-Instruct",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

# å¾®è°ƒå‚æ•°åˆå§‹åŒ–
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""<a name="Data"></a>
### Data Prep
We now use the Alpaca dataset from [vicgalle](https://huggingface.co/datasets/vicgalle/alpaca-gpt4), which is a version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html) generated from GPT4. You can replace this code section with your own data prep.
"""

# è£…è½½æ•°æ®é›†
from datasets import load_dataset
dataset = load_dataset("llm-wizard/alpaca-gpt4-data-zh", split = "train")
print(dataset.column_names)

"""One issue is this dataset has multiple columns. For `Ollama` and `llama.cpp` to function like a custom `ChatGPT` Chatbot, we must only have 2 columns - an `instruction` and an `output` column."""

# æ‰“å°æ•°æ®é›†åˆ—
print(dataset.column_names)

"""To solve this, we shall do the following:
* Merge all columns into 1 instruction prompt.
* Remember LLMs are text predictors, so we can customize the instruction to anything we like!
* Use the `to_sharegpt` function to do this column merging process!

For example below in our [Titanic CSV finetuning notebook](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing), we merged multiple columns in 1 prompt:

<img src="https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Merge.png" height="100">

To merge multiple columns into 1, use `merged_prompt`.
* Enclose all columns in curly braces `{}`.
* Optional text must be enclused in `[[]]`. For example if the column "Pclass" is empty, the merging function will not show the text and skp this. This is useful for datasets with missing values.
* You can select every column, or a few!
* Select the output or target / prediction column in `output_column_name`. For the Alpaca dataset, this will be `output`.

To make the finetune handle multiple turns (like in ChatGPT), we have to create a "fake" dataset with multiple turns - we use `conversation_extension` to randomnly select some conversations from the dataset, and pack them together into 1 conversation.
"""

# æ•°æ®merge
from unsloth import to_sharegpt
dataset = to_sharegpt(
    dataset,
    merged_prompt = "{instruction}[[\nYour input is:\n{input}]]",
    output_column_name = "output",
    conversation_extension = 0, # Select more to handle longer conversations
)

# æ‰“å°æ•°æ®åˆ—ã€æ•°æ®é›†çš„ç¬¬ä¸€ç»„å¾®è°ƒæ•°æ®
print(dataset.column_names)
print(dataset[1])

"""Finally use `standardize_sharegpt` to fix up the dataset!"""

# æ•°æ®ä¿®æ•´
from unsloth import standardize_sharegpt
dataset = standardize_sharegpt(dataset)
print(dataset[1])

"""### Customizable Chat Templates

You also need to specify a chat template. Previously, you could use the Alpaca format as shown below.

The issue is the Alpaca format has 3 fields, whilst OpenAI style chatbots must only use 2 fields (instruction and response). That's why we used the `to_sharegpt` function to merge these columns into 1.
"""

# å®šä¹‰æç¤ºè¯æ¨¡æ¿ ç”Ÿæˆåˆæ­¥prompt
chat_template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>

{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{OUTPUT}<|eot_id|>"""

from unsloth import apply_chat_template
dataset = apply_chat_template(
    dataset,
    tokenizer = tokenizer,
    chat_template = chat_template,
    default_system_message = "You are a helpful assistant",
)

print(dataset[1])

"""<a name="Train"></a>
### Train the model
Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!
"""

# å¾®è°ƒå¼•æ“åˆå§‹åŒ–
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        # num_train_epochs = 1, # For longer training runs!
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

#@title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

# æ‰§è¡Œå¾®è°ƒ
trainer_stats = trainer.train()

#@title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Inference
Let's run the model! Unsloth makes inference natively 2x faster as well! You should use prompts which are similar to the ones you had finetuned on, otherwise you might get bad results!
"""

# æ¨ç†æµ‹è¯•
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
messages = [                    # Change below!
    {"role": "user", "content": "Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8,"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

# æ¨ç†æµ‹è¯•
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
messages = [                    # Change below!
    {"role": "user", "content": "ä¸‰åŸè‰²æ˜¯ä»€ä¹ˆï¼Ÿä½ æ˜¯è°ï¼Ÿ"}
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""Since we created an actual chatbot, you can also do longer conversations by manually adding alternating conversations between the user and assistant!"""

# æ¨ç†æµ‹è¯•
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
messages = [                         # Change below!
    {"role": "user",      "content": "Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8"},
    {"role": "assistant", "content": "The fibonacci sequence continues as 13, 21, 34, 55 and 89."},
    {"role": "user",      "content": "What is France's tallest tower called?"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""<a name="Save"></a>
### Saving, loading finetuned models
To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.

**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!
"""

# loraæ¨¡å‹æŒä¹…åŒ–
model.save_pretrained("lora_model") # Local saving
tokenizer.save_pretrained("lora_model")
# model.push_to_hub("your_name/lora_model", token = "...") # Online saving
# tokenizer.push_to_hub("your_name/lora_model", token = "...") # Online saving

"""Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"""

# ä½¿ç”¨unslothå†…ç½®å¼•æ“ï¼Œå¯¹loraæ¨¡å‹æ¨ç†
if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference
pass

messages = [                    # Change below!
    {"role": "user", "content": "Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."""

# ä½¿ç”¨huggingfaceå¼•æ“ï¼Œå¯¹loraæ¨¡å‹æ¨ç†
if False:
    # I highly do NOT suggest - use Unsloth if possible
    from peft import AutoPeftModelForCausalLM
    from transformers import AutoTokenizer
    model = AutoPeftModelForCausalLM.from_pretrained(
        "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        load_in_4bit = load_in_4bit,
    )
    tokenizer = AutoTokenizer.from_pretrained("lora_model")

"""<a name="Ollama"></a>
### Ollama Support

[Unsloth](https://github.com/unslothai/unsloth) now allows you to automatically finetune and create a [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md), and export to [Ollama](https://ollama.com/)! This makes finetuning much easier and provides a seamless workflow from `Unsloth` to `Ollama`!

Let's first install `Ollama`!
"""

# å®‰è£…ollama
if False:
  !curl -fsSL https://ollama.com/install.sh | sh

"""Next, we shall save the model to GGUF / llama.cpp

We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.

Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
* `q8_0` - Fast conversion. High resource use, but generally acceptable.
* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.
* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.

We also support saving to multiple GGUF options in a list fashion! This can speed things up by 10 minutes or more if you want multiple export formats!
"""

# å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜åˆ°ç¡¬ç›˜ï¼Œå¹¶ä¸Šä¼ åˆ°hugging face
from google.colab import userdata


hug_token = userdata.get('hug_token')

hug_model = "LucasInsight/Meta-Llama-3.1-8B-Instruct"

# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if True: model.push_to_hub_gguf(hug_model, tokenizer, token = hug_token)

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf(hug_model, tokenizer, quantization_method = "f16", token = "")

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf(hug_model, tokenizer, quantization_method = "q4_k_m", token = hug_token)

# Save to multiple GGUF options - much faster if you want multiple!
if True:
    model.push_to_hub_gguf(
        hug_model, # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "", # Get a token at https://huggingface.co/settings/tokens
    )

"""We use `subprocess` to start `Ollama` up in a non blocking fashion! In your own desktop, you can simply open up a new `terminal` and type `ollama serve`, but in Colab, we have to use this hack!"""

# å¯åŠ¨ollama
if False:
  import subprocess
  subprocess.Popen(["ollama", "serve"])
  import time
  time.sleep(3) # Wait for a few seconds for Ollama to load!

"""`Ollama` needs a `Modelfile`, which specifies the model's prompt format. Let's print Unsloth's auto generated one:"""

# æ‰“å°ollama Modelfile
print(tokenizer._ollama_modelfile)

"""We now will create an `Ollama` model called `unsloth_model` using the `Modelfile` which we auto generated!"""

# ollamaè£…è½½å¾®è°ƒåçš„æ¨¡å‹
# !ollama create unsloth_model -f ./model/Modelfile

"""And now we can do inference on it via `Ollama`!

You can also upload to `Ollama` and try the `Ollama` Desktop app by heading to https://www.ollama.com/
"""

# è°ƒç”¨ollamaè£…è½½çš„å¾®è°ƒåçš„æ¨¡å‹chatæ¥å£
if False:
  !curl http://localhost:11434/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "unsloth_model", \
      "messages": [ \
        { \
          "role": "system", \
          "content": "ä½ æ˜¯LucasInsightçš„æ•°å­—åˆ†èº«ã€‚" \
        }, \
        { \
          "role": "user", \
          "content": "ä½ æ˜¯è°ï¼Ÿ" \
        } \
      ] \
    }'

# è°ƒç”¨ollamaè£…è½½çš„å¾®è°ƒåçš„æ¨¡å‹chatæ¥å£

if False:
  !curl http://localhost:11434/api/chat -d '{ \
      "model": "unsloth_model", \
      "messages": [ \
          { "role": "user", "content": "Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8," } \
      ] \
      }'

"""æœ¬æœŸè§†é¢‘åšåˆ°è¿™é‡Œï¼Œæ¬¢è¿å¤§å®¶ç‚¹èµå…³æ³¨ã€æ”¶è—å’Œè½¬å‘ã€‚æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ç¤¾ç¾¤ï¼Œæœ¬æœŸè¯¾ç¨‹æ¶‰åŠåˆ°çš„ä»£ç ï¼Œæˆ‘ä¼šæ”¾åˆ°è§†é¢‘ä¸‹æ–¹çš„é“¾æ¥ã€‚

è°¢è°¢å¤§å®¶çš„è§‚çœ‹ï¼

# ChatGPT interactive mode

### â­ To run the finetuned model like in a ChatGPT style interface, first click the **| >_ |** button.
![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Where_Terminal.png)

---
---
---

### â­ Then, type `ollama run unsloth_model`

![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Terminal_Type.png)

---
---
---
### â­ And you have a CHatGPT style assistant!

### Type any question you like and press `ENTER`. If you want to exit, hit `CTRL + D`
![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Assistant.png)

And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Try our [Ollama CSV notebook](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) to upload CSVs for finetuning!

Some other links:
1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)
2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)
3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)
4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)
5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ğŸ¤— HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!
7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)
8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)

<div class="align-center">
  <a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://ollama.com/"><img src="https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/ollama.png" height="44"></a>
  <a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
  <a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Join Discord if you need help + â­ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> â­
</div>
"""